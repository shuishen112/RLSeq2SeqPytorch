{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_article = pd.read_csv(\"data/unfinished/train.art.shuf1000.txt\", sep = \"\\t\", names = [\"article\"])\n",
    "df_abstract = pd.read_csv(\"data/unfinished/train.abs.shuf1000.txt\", sep = \"\\t\", names = [\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_article))\n",
    "print(len(df_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_article['article'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two more &lt;unk&gt; pan am drug tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>argentine basketball completes olympic prepara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>media watchdog concerned about gambia press trial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lynyrd skynyrd graves vandalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cingular and verizon 's good old-fashioned ad war</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract\n",
       "0                   two more <unk> pan am drug tests\n",
       "1  argentine basketball completes olympic prepara...\n",
       "2  media watchdog concerned about gambia press trial\n",
       "3                   lynyrd skynyrd graves vandalized\n",
       "4  cingular and verizon 's good old-fashioned ad war"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abstract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#  bulid vocabulary\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(df_list):\n",
    "    for df in df_list:\n",
    "        for text in df:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens([df_abstract['abstract'].to_list(),df_article['article'].to_list()]), specials=['[UNK]','[PAD]','[START]','[STOP]'])\n",
    "vocab.set_default_index(vocab['[UNK]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__getitem__(\"[START]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 21:51:36.657484: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 21:51:38.252423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 21:51:38.252566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 21:51:38.252580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from data_util import data\n",
    "from data_util import config\n",
    "\n",
    "class Example(object):\n",
    "    def __init__(self, article, abstract_sentences, vocab):\n",
    "        # Get ids of special tokens\n",
    "        start_decoding = vocab.__getitem__(data.START_DECODING)\n",
    "        stop_decoding = vocab.__getitem__(data.STOP_DECODING)\n",
    "\n",
    "        # Process the article\n",
    "        article_words = article.split()\n",
    "        if len(article_words) > config.max_enc_steps:\n",
    "            article_words = article_words[: config.max_enc_steps]\n",
    "        self.enc_len = len(\n",
    "            article_words\n",
    "        )  # store the length after truncation but before padding\n",
    "        self.enc_input = [\n",
    "            vocab.__getitem__(w) for w in article_words\n",
    "        ]  # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Process the abstract\n",
    "        abstract = \" \".join(abstract_sentences)  # string\n",
    "        abstract_words = abstract.split()  # list of strings\n",
    "        abs_ids = [\n",
    "            vocab.__getitem__(w) for w in abstract_words\n",
    "        ]  # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Get the decoder input sequence and target sequence\n",
    "        self.dec_input, _ = self.get_dec_inp_targ_seqs(\n",
    "            abs_ids, config.max_dec_steps, start_decoding, stop_decoding\n",
    "        )\n",
    "        self.dec_len = len(self.dec_input)\n",
    "\n",
    "        # If using pointer-generator mode, we need to store some extra info\n",
    "        # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
    "        self.enc_input_extend_vocab, self.article_oovs = data.article2ids_new(\n",
    "            article_words, vocab\n",
    "        )\n",
    "\n",
    "        # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "        abs_ids_extend_vocab = data.abstract2ids_new(\n",
    "            abstract_words, vocab, self.article_oovs\n",
    "        )\n",
    "\n",
    "        # Get decoder target sequence\n",
    "        _, self.target = self.get_dec_inp_targ_seqs(\n",
    "            abs_ids_extend_vocab, config.max_dec_steps, start_decoding, stop_decoding\n",
    "        )\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article = article\n",
    "        self.original_abstract = abstract\n",
    "        self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "    def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
    "        inp = [start_id] + sequence[:]\n",
    "        target = sequence[:]\n",
    "        if len(inp) > max_len:  # truncate\n",
    "            inp = inp[:max_len]\n",
    "            target = target[:max_len]  # no end_token\n",
    "        else:  # no truncation\n",
    "            target.append(stop_id)  # end token\n",
    "        assert len(inp) == len(target)\n",
    "        return inp, target\n",
    "\n",
    "    def pad_decoder_inp_targ(self, max_len, pad_id):\n",
    "        while len(self.dec_input) < max_len:\n",
    "            self.dec_input.append(pad_id)\n",
    "        while len(self.target) < max_len:\n",
    "            self.target.append(pad_id)\n",
    "\n",
    "    def pad_encoder_input(self, max_len, pad_id):\n",
    "        while len(self.enc_input) < max_len:\n",
    "            self.enc_input.append(pad_id)\n",
    "        while len(self.enc_input_extend_vocab) < max_len:\n",
    "            self.enc_input_extend_vocab.append(pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class S2SDataset(Dataset):\n",
    "    def __init__(self,):\n",
    "        self.df_article = pd.read_csv(\"data/unfinished/train.art.shuf1000.txt\", sep = \"\\t\", names = [\"article\"])\n",
    "        self.df_abstract = pd.read_csv(\"data/unfinished/train.abs.shuf1000.txt\", sep = \"\\t\", names = [\"abstract\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_abstract)\n",
    "    def __getitem__(self, index):\n",
    "        article = self.df_article[\"article\"].iloc[index]\n",
    "        abstract = self.df_abstract['abstract'].iloc[index]\n",
    "        ex = Example(article, [abstract], vocab)\n",
    "        return ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the dataset\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "class Batch(object):\n",
    "    def __init__(self, example_list, vocab, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_id = vocab.__getitem__(\n",
    "            data.PAD_TOKEN\n",
    "        )  # id of the PAD token used to pad sequences\n",
    "        self.init_encoder_seq(example_list)  # initialize the input to the encoder\n",
    "        self.init_decoder_seq(\n",
    "            example_list\n",
    "        )  # initialize the input and targets for the decoder\n",
    "        self.store_orig_strings(example_list)  # store the original strings\n",
    "\n",
    "    def init_encoder_seq(self, example_list):\n",
    "        # Determine the maximum length of the encoder input sequence in this batch\n",
    "        max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "\n",
    "        # Pad the encoder input sequences up to the length of the longest sequence\n",
    "        for ex in example_list:\n",
    "            ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays\n",
    "        # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
    "        self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "        self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "        self.enc_padding_mask = np.zeros(\n",
    "            (self.batch_size, max_enc_seq_len), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.enc_batch[i, :] = ex.enc_input[:]\n",
    "            self.enc_lens[i] = ex.enc_len\n",
    "            for j in range(ex.enc_len):\n",
    "                self.enc_padding_mask[i][j] = 1\n",
    "\n",
    "        # For pointer-generator mode, need to store some extra info\n",
    "        # Determine the max number of in-article OOVs in this batch\n",
    "        self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "        # Store the in-article OOVs themselves\n",
    "        self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "        # Store the version of the enc_batch that uses the article OOV ids\n",
    "        self.enc_batch_extend_vocab = np.zeros(\n",
    "            (self.batch_size, max_enc_seq_len), dtype=np.int32\n",
    "        )\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
    "\n",
    "    def init_decoder_seq(self, example_list):\n",
    "        # Pad the inputs and targets\n",
    "        for ex in example_list:\n",
    "            ex.pad_decoder_inp_targ(config.max_dec_steps, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays.\n",
    "        self.dec_batch = np.zeros(\n",
    "            (self.batch_size, config.max_dec_steps), dtype=np.int32\n",
    "        )\n",
    "        self.target_batch = np.zeros(\n",
    "            (self.batch_size, config.max_dec_steps), dtype=np.int32\n",
    "        )\n",
    "        # self.dec_padding_mask = np.zeros((self.batch_size, config.max_dec_steps), dtype=np.float32)\n",
    "        self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.dec_batch[i, :] = ex.dec_input[:]\n",
    "            self.target_batch[i, :] = ex.target[:]\n",
    "            self.dec_lens[i] = ex.dec_len\n",
    "            # for j in range(ex.dec_len):\n",
    "            #   self.dec_padding_mask[i][j] = 1\n",
    "\n",
    "    def store_orig_strings(self, example_list):\n",
    "        self.original_articles = [\n",
    "            ex.original_article for ex in example_list\n",
    "        ]  # list of lists\n",
    "        self.original_abstracts = [\n",
    "            ex.original_abstract for ex in example_list\n",
    "        ]  # list of lists\n",
    "        self.original_abstracts_sents = [\n",
    "            ex.original_abstract_sents for ex in example_list\n",
    "        ]  # list of list of lists\n",
    "\n",
    "def collate_batch(batch):\n",
    "\n",
    "    return Batch(batch, vocab, len(batch))\n",
    "    \n",
    "    # we need to encoding the d\n",
    "\n",
    "    # we need to padding hte dataset for the max length\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = S2SDataset()\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_batch, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "def get_cuda(tensor):\n",
    "    if T.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def get_enc_data(batch):\n",
    "    batch_size = len(batch.enc_lens)\n",
    "    enc_batch = T.from_numpy(batch.enc_batch).long()\n",
    "    enc_padding_mask = T.from_numpy(batch.enc_padding_mask).float()\n",
    "\n",
    "    enc_lens = batch.enc_lens\n",
    "\n",
    "    ct_e = T.zeros(batch_size, 2*config.hidden_dim)\n",
    "\n",
    "    enc_batch = get_cuda(enc_batch)\n",
    "    enc_padding_mask = get_cuda(enc_padding_mask)\n",
    "\n",
    "    ct_e = get_cuda(ct_e)\n",
    "\n",
    "    enc_batch_extend_vocab = None\n",
    "    if batch.enc_batch_extend_vocab is not None:\n",
    "        enc_batch_extend_vocab = T.from_numpy(batch.enc_batch_extend_vocab).long()\n",
    "        enc_batch_extend_vocab = get_cuda(enc_batch_extend_vocab)\n",
    "\n",
    "    extra_zeros = None\n",
    "    if batch.max_art_oovs > 0:\n",
    "        extra_zeros = T.zeros(batch_size, batch.max_art_oovs)\n",
    "        extra_zeros = get_cuda(extra_zeros)\n",
    "\n",
    "\n",
    "    return enc_batch, enc_lens, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, ct_e\n",
    "\n",
    "\n",
    "def get_dec_data(batch):\n",
    "    dec_batch = T.from_numpy(batch.dec_batch).long()\n",
    "    dec_lens = batch.dec_lens\n",
    "    max_dec_len = np.max(dec_lens)\n",
    "    dec_lens = T.from_numpy(batch.dec_lens).float()\n",
    "\n",
    "    target_batch = T.from_numpy(batch.target_batch).long()\n",
    "\n",
    "    dec_batch = get_cuda(dec_batch)\n",
    "    dec_lens = get_cuda(dec_lens)\n",
    "    target_batch = get_cuda(target_batch)\n",
    "\n",
    "    return dec_batch, max_dec_len, dec_lens, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  8.,  8.,  5.,  9.,  6.,  7.,  9.,  8.,  5.,  6., 11.,  7., 11.,\n",
       "        12., 10.,  7.,  8., 14., 14.,  7.,  7.,  8.,  8., 11.,  8.,  8., 13.,\n",
       "         7.,  5., 10.,  9.,  7.,  9., 12.,  5.,  9.,  8., 12., 10.,  8.,  9.,\n",
       "         9., 15.,  9., 12.,  6.,  9.,  7.,  8., 11.,  8.,  8., 11., 12., 15.,\n",
       "         7.,  8.,  7.,  8.,  9.,  8.,  9., 13.], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from model import Model\n",
    "\n",
    "class pl_model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # get the model \n",
    "        self.model = Model()\n",
    "        self.vocab = vocab\n",
    "        self.start_id = self.vocab.__getitem__(data.START_DECODING)\n",
    "        self.end_id = self.vocab.__getitem__(data.STOP_DECODING)\n",
    "        self.pad_id = self.vocab.__getitem__(data.PAD_TOKEN)\n",
    "        self.unk_id = self.vocab.__getitem__(data.UNKNOWN_TOKEN)\n",
    "    def forward(self, enc_batch, enc_lens):\n",
    "        enc_batch = self.model.embeds(enc_batch)  # Get embeddings for encoder input\n",
    "        enc_out, enc_hidden = self.model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        return enc_out, enc_hidden\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        (\n",
    "            enc_batch,\n",
    "            enc_lens,\n",
    "            enc_padding_mask,\n",
    "            enc_batch_extend_vocab,\n",
    "            extra_zeros,\n",
    "            context,\n",
    "        ) = get_enc_data(batch)\n",
    "        \n",
    "        enc_out, enc_hidden = self(enc_batch, enc_lens)\n",
    "        dec_batch, max_dec_len, dec_lens, target_batch = get_dec_data(batch)\n",
    "\n",
    "        # Get input and target batchs for training decoder\n",
    "        step_losses = []\n",
    "        s_t = (enc_hidden[0], enc_hidden[1])  # Decoder hidden states\n",
    "        x_t = get_cuda(\n",
    "            T.LongTensor(len(enc_out)).fill_(self.start_id)\n",
    "        )  # Input to the decoder\n",
    "        prev_s = None  # Used for intra-decoder attention (section 2.2 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        sum_temporal_srcs = None  # Used for intra-temporal attention (section 2.1 in https://arxiv.org/pdf/1705.04304.pdf)\n",
    "        for t in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            use_gound_truth = get_cuda(\n",
    "                (T.rand(len(enc_out)) > 0.25)\n",
    "            ).long()  # Probabilities indicating whether to use ground truth labels instead of previous decoded tokens\n",
    "            x_t = (\n",
    "                use_gound_truth * dec_batch[:, t] + (1 - use_gound_truth) * x_t\n",
    "            )  # Select decoder input based on use_ground_truth probabilities\n",
    "            x_t = self.model.embeds(x_t)\n",
    "            final_dist, s_t, ct_e, sum_temporal_srcs, prev_s = self.model.decoder(\n",
    "                x_t,\n",
    "                s_t,\n",
    "                enc_out,\n",
    "                enc_padding_mask,\n",
    "                context,\n",
    "                extra_zeros,\n",
    "                enc_batch_extend_vocab,\n",
    "                sum_temporal_srcs,\n",
    "                prev_s,\n",
    "            )\n",
    "            target = target_batch[:, t]\n",
    "            log_probs = T.log(final_dist + config.eps)\n",
    "            step_loss = F.nll_loss(\n",
    "                log_probs, target, reduction=\"none\", ignore_index=self.pad_id\n",
    "            )\n",
    "            step_losses.append(step_loss)\n",
    "            x_t = T.multinomial(\n",
    "                final_dist, 1\n",
    "            ).squeeze()  # Sample words from final distribution which can be used as input in next time step\n",
    "            is_oov = (\n",
    "                x_t >= config.vocab_size\n",
    "            ).long()  # Mask indicating whether sampled word is OOV\n",
    "            x_t = (1 - is_oov) * x_t.detach() + (\n",
    "                is_oov\n",
    "            ) * self.unk_id  # Replace OOVs with [UNK] token\n",
    "\n",
    "        losses = T.sum(\n",
    "            T.stack(step_losses, 1), 1\n",
    "        )  # unnormalized losses for each example in the batch; (batch_size)\n",
    "        batch_avg_loss = losses / dec_lens  # Normalized losses; (batch_size)\n",
    "        mle_loss = T.mean(batch_avg_loss)  # Average batch loss\n",
    "        return mle_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.02)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /maps/projects/futhark1/data/wzm289/code/Text-Summarizer-Pytorch/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 48.2 M\n",
      "1 | vocab | Vocab | 0     \n",
      "--------------------------------\n",
      "48.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "48.2 M    Total params\n",
      "192.939   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 16/16 [00:02<00:00,  7.66it/s, loss=15.7, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "pl_model = pl_model()\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(pl_model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d0e3ecb0baebba56a7f46a8b4a51e9efd4d37834ea1e239c938a302ce08aa9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
