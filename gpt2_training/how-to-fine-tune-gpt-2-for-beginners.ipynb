{"cells":[{"cell_type":"markdown","metadata":{},"source":["# GPT-2 Fine-Tuning"]},{"cell_type":"markdown","metadata":{},"source":["#### This is the code I wrote at the company, but I think it would be nice to share it here, so I post it.\n","\n","#### With this data, we will fine tune GPT-2 to make a sentence generation model. \n","\n","#### This code is for AI beginners."]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Data preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["#### the data contains unnecessary newlines, tags, and URLs it will be necessary to remove them before preprocessing."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def cleaning(s):\n","    s = str(s)\n","    s = re.sub('\\s\\W',' ',s)\n","    s = re.sub('\\W,\\s',' ',s)\n","    s = re.sub(\"\\d+\", \"\", s)\n","    s = re.sub('\\s+',' ',s)\n","    s = re.sub('[!@#$_]', '', s)\n","    s = s.replace(\"co\",\"\")\n","    s = s.replace(\"https\",\"\")\n","    s = s.replace(\"[\\w*\",\" \")\n","    return s"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"Articles.csv\", encoding=\"ISO-8859-1\") \n","df = df.dropna()\n","text_data = open('Articles.txt', 'w')\n","for idx, item in df.iterrows():\n","  article = cleaning(item[\"Article\"])\n","  text_data.write(article)\n","text_data.close()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Model Training"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# !pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2022-12-27 15:39:46.320967: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-27 15:39:50.374674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2022-12-27 15:39:50.374794: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2022-12-27 15:39:50.374806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","train_data = load_dataset(\"./Articles.txt\", tokenizer)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([   42,  1503, 16219,    40,    25,   383, 44830,    71,  1230,   468,\n","         3066,   284,  2222,   866,  1171,  4839, 33164,   416,   583,  1247,\n","         2233,   284,  4858,  7741,   287, 30918,  1720,  4536,   416,   262,\n","         2717,  1230,    11, 32960,  3000,  2098,    13, 21188,   531,  7741,\n","          287, 33164,   481,   307,  9723,   319,  1171,  4839,    11,   374,\n","         3378, 26615,    11, 17536,   290,   584,  1724,   286, 11300,    13,\n","        10294,    11, 46154, 19940,   632,   660, 18108,   509, 25621,     8,\n","          468,  6520,   284, 27851,   416,   262,  1230,  2551,    13,    42,\n","        25621,  1992,  5686,  1477,   324, 36810, 49573,   531,   262,  8085,\n","         5843,   389,  5047,   262,  9016, 33164,   287, 46154,   355, 29034,\n","          533,   284,   584,  3354,   286,   262,  1418,   563,    11,  4375,\n","          326, 40653,  5672,  1057,   319,  3082,  2790, 12068, 14345,   327,\n","        10503,   737, 36810, 49573,   531, 46154,  1007,  1819])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(train_data))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["128"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(next(iter(train_data)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# test DataCollatorForLanguageModeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, \n","        mlm=mlm,\n","    )\n","    return data_collator\n","\n","\n","def train(train_file_path,model_name,\n","          output_dir,\n","          overwrite_output_dir,\n","          per_device_train_batch_size,\n","          num_train_epochs,\n","          save_steps):\n","  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","  train_dataset = load_dataset(train_file_path, tokenizer)\n","  data_collator = load_data_collator(tokenizer)\n","\n","  tokenizer.save_pretrained(output_dir)\n","      \n","  model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","  model.save_pretrained(output_dir)\n","\n","  training_args = TrainingArguments(\n","          output_dir=output_dir,\n","          overwrite_output_dir=overwrite_output_dir,\n","          per_device_train_batch_size=per_device_train_batch_size,\n","          num_train_epochs=num_train_epochs,\n","      )\n","\n","  trainer = Trainer(\n","          model=model,\n","          args=training_args,\n","          data_collator=data_collator,\n","          train_dataset=train_dataset,\n","  )\n","      \n","  trainer.train()\n","  trainer.save_model()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'data_collator' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_collator:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(data)\n","\u001b[0;31mNameError\u001b[0m: name 'data_collator' is not defined"]}],"source":[]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# you need to set parameters \n","train_file_path = \"./Articles.txt\"\n","model_name = 'gpt2'\n","output_dir = './result'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 5.0\n","save_steps = 500"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 8024\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2510\n","  Number of trainable parameters = 124439808\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhansu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/maps/projects/futhark1/data/wzm289/code/RLSeq2SeqPytorch/wandb/run-20221227_104143-33u68jvt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/zhansu/huggingface/runs/33u68jvt\" target=\"_blank\">./result</a></strong> to <a href=\"https://wandb.ai/zhansu/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2510' max='2510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2510/2510 17:56, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.610600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.242200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.087000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.992600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>2.937600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./result/checkpoint-500\n","Configuration saved in ./result/checkpoint-500/config.json\n","Model weights saved in ./result/checkpoint-500/pytorch_model.bin\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Saving model checkpoint to ./result/checkpoint-1000\n","Configuration saved in ./result/checkpoint-1000/config.json\n","Model weights saved in ./result/checkpoint-1000/pytorch_model.bin\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Saving model checkpoint to ./result/checkpoint-1500\n","Configuration saved in ./result/checkpoint-1500/config.json\n","Model weights saved in ./result/checkpoint-1500/pytorch_model.bin\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Saving model checkpoint to ./result/checkpoint-2000\n","Configuration saved in ./result/checkpoint-2000/config.json\n","Model weights saved in ./result/checkpoint-2000/pytorch_model.bin\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Saving model checkpoint to ./result/checkpoint-2500\n","Configuration saved in ./result/checkpoint-2500/config.json\n","Model weights saved in ./result/checkpoint-2500/pytorch_model.bin\n","/home/wzm289/miniconda3/envs/ACL/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./result\n","Configuration saved in ./result/config.json\n","Model weights saved in ./result/pytorch_model.bin\n"]}],"source":["# It takes about 30 minutes to train in colab.\n","train(\n","    train_file_path=train_file_path,\n","    model_name=model_name,\n","    output_dir=output_dir,\n","    overwrite_output_dir=overwrite_output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    num_train_epochs=num_train_epochs,\n","    save_steps=save_steps\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Inference"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def load_model(model_path):\n","    model = GPT2LMHeadModel.from_pretrained(model_path)\n","    return model\n","\n","\n","def load_tokenizer(tokenizer_path):\n","    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n","    return tokenizer\n","\n","\n","def generate_text(sequence, max_length):\n","    model_path = \"./result\"\n","    model = load_model(model_path)\n","    tokenizer = load_tokenizer(model_path)\n","    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n","    final_outputs = model.generate(\n","        ids,\n","        do_sample=False,\n","        max_length=max_length,\n","        pad_token_id=model.config.eos_token_id,\n","        top_k=50,\n","        top_p=0.95,\n","    )\n","    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file ./result/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","loading weights file ./result/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./result.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","loading file vocab.json\n","loading file merges.txt\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n"]},{"name":"stdout","output_type":"stream","text":["oil price, which has been on a downward trajectory since the start of the year, has fallen to\n"]}],"source":["# sequence = input() # oil price\n","# max_len = int(input()) # 20\n","generate_text(\"oil price\", 20) # oil price for July June which had been low at as low as was originally stated Prices have since resumed"]},{"cell_type":"markdown","metadata":{},"source":["The following process may be a little more complicated or tedious because you have to write the code one by one, and it takes a long time if you don't have a personal GPU.\n","\n","Then, how about use Ainize's Teachable NLP? Teachable NLP provides an API to use the model so when data is input it will automatically learn quickly.\n","\n","Teachable NLP : [https://ainize.ai/teachable-nlp](https://link.ainize.ai/3tJVRD1)\n","\n","Teachable NLP Tutorial : [https://forum.ainetwork.ai/t/teachable-nlp-how-to-use-teachable-nlp/65](https://link.ainize.ai/3tATaUh)"]}],"metadata":{"kernelspec":{"display_name":"ACL","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"8d0e3ecb0baebba56a7f46a8b4a51e9efd4d37834ea1e239c938a302ce08aa9c"}}},"nbformat":4,"nbformat_minor":4}
